{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebook, we have shown how to [fit a psychometric curve using pyTorch](https://laurentperrinet.github.io/sciblog/posts/2020-04-08-fitting-a-psychometric-curve-using-pytorch.html). Here, I would like to review some nice properies of the logistic regression model (WORK IN PROGRESS). \n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# quelques définitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation du notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons importer quelques librairies qui seront utilisées pour la manipulation numérique de matrices et l'affichage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des variables et fonctions\n",
    "\n",
    "... utiles pour réaliser une classification  de données synthétiques selon une régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256 # nombre de valeurs de l'entrée\n",
    "n_classes = 10 # nombre de classes\n",
    "N_batch = 100 # nombre d'échantillons\n",
    "seed = 1973\n",
    "\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(N+1, n_classes) # FIXED design matrix of weights (w) according to classes\n",
    "\n",
    "# Definition de l'évidence (e) \n",
    "def evidence(W, X):\n",
    "    \"\"\"\n",
    "    Modèle génératif de l'évidence:\n",
    "    W[:-1, :] = N poids pour n_classes\n",
    "    W[-1, :] = 1 biais pour chaque classe\n",
    "    \"\"\"\n",
    "    e = (X @ W[:-1, :]) + W[-1, :] \n",
    "    return e\n",
    "\n",
    "# Transformation de l'entrée (e) en probabilité (p) selon une courbe de regression logistique\n",
    "def sigmoid(logodds):\n",
    "    return 1 / (1 + np.exp(-logodds))\n",
    "\n",
    "def get_data(W, seed, N_batch):\n",
    "    N, n_classes = W.shape[0]-1, W.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    # nos entrées (X) : n_batch échantillons comprenant chacun N valeurs\n",
    "    # tirage de causes aléatoires\n",
    "    X = np.random.randn(N_batch, N) \n",
    "    # modèle génératif: mélange\n",
    "    p = sigmoid(evidence(W, X))\n",
    "    # Tirage de Bernoulli : si p > à random => 1, si p < à random = 0 \n",
    "    y = p > np.random.rand(N_batch, n_classes) # Bernoulli, generate data \n",
    "    return X, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, p, y = get_data(W, seed, N_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec __X,p,y = get_data(W, seed, N_batch)__ on appelle la fct get_data, qui appelle la fonction `sigmoid`, qui appelle la fonction evidence afin de nous renvoyer les entrées X (100 entrées, comprenant chacune 256 valeures), les probabilités de classification de ces différentes entrées dans les différentes classes (p), et la classification après tirage de bernoulli (y = succès si p > 0.5, echcec si p < 0.5). Pour visualiser les sorties (p et y) en fonction des entrées (X et e), on les stock dans des arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de p en fonction de e pour une classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37124/920322755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(e[:,1],p[:,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de p en fonction de e pour une classe + de la sortie (tirage de Bernoulli = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37124/1408648590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#proba de e d'être classifié dans classe 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"red\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Sortie de Bernoulli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(e[:,1],p[:,1], alpha=.2) #proba de e d'être classifié dans classe 1\n",
    "plt.scatter(e[:,1],y[:,1], alpha=.2, marker=\"x\", color=\"red\") #Sortie de Bernoulli "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de la distribution des probabilités de classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p.ravel(), bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de p en fonction de chaque entrée X + de la sortie y  'permise' par l'ajout des w et biais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.average(X.reshape(-1, 256), axis=1)\n",
    "plt.scatter(X_1,p[:,1]) \n",
    "plt.scatter(X_1,y[:,1],marker=\"x\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... la situation est plus confuse... est-ce quon va arriver à distinguer les différents cas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau d'inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "# see https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html#pytorch-loss-input-confusion-cheatsheet\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\") # loss divided by output size//the sum of the output will be divided by the number of elements in the output\n",
    "#criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output \n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    \n",
    "    # voir ce que signifie self \n",
    "    # voir ce que signifie super \n",
    "    \n",
    "    def __init__(self, N, n_classes, bias=True): # (self, input_dim, output_dim)\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        \n",
    "        # 1ere couche : self.linear = torch.nn.Linear(input_dim, output_dim, bias=true) bias = true pour apprentissage\n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias) \n",
    "        \n",
    "        # 2e couche : self.nl = torch.nn.Sigmoid() = ouputs \n",
    "        # autre possibilité : self.nl = torch.nn.LogSoftmax(n_classes)\n",
    "        self.nl = torch.nn.Sigmoid()\n",
    "    \n",
    "    # maintenant qu'on a difinit les couches, on \n",
    "    # self.nl me donne la sigmoïde de e \n",
    "    # X = (N, n_batch) là on est dans 1 batch donc X = 256 = N \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.nl(self.linear(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __init__ permet de définir notre NN, les couches, leur role, la dimention des entrées et sorties\n",
    " \n",
    "On a un réseau composé de deux couches : \n",
    "- la première (self.linear) réalise une transformation linéaire de nos entrées X (de dimention N =  256) de sorte à ce que la sortie y = (w*x)+biais = e (de dimention n_classes = 10). Une entrée sera associée aux différentes classes avec un w et un biais associé pour chacune des classes. Après plusieurs batch, on obtiendra plusieurs e pour la même ent alors plusieurs e pour chaque classes.\n",
    "    \n",
    "- la deuxième (self.nl) transforme ces sorties (e) pour leur donner une distribution sigmoïde. On obtient ainsi la probabilité de classification de l'entrée X, associée à un poid w et au biais. On aura n_classes sigmoïdes, représentant la probabilité de classification de l'entrée dans chacune des classes en fonction du w et du biais associé. \n",
    "\n",
    "La fonction __forward__ permet de construire le réseau en connectant les couches selon un modèle forward : self.nl(self.linear(X))\n",
    "\n",
    "Par la suite, le réseau va modifier ses paramètres : w et biais de sorte à ce que la classification des X se fasse de la même manière que pendant l'initialisation\n",
    "\n",
    "-> Pour cela, on demande à notre réseau de calculer la loss function entre la classification réalisée par self.nl(self.linear(X)) et celle réalisée par y \n",
    "\n",
    "-> Plus la différence est grande, plus la loss function l'est aussi et plus le réseau va modifier ses paramètres (W et biais) pour classifier de la même façon que pdt l'initialisation \n",
    "\n",
    "-> On devrait donc retrouver à la fin des W et des biais similaires entre ceux fixés par nous pendant l'initialisation et ceux déterminés par le réseau pdt l'inversion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegressionModel' object has no attribute 'summmary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37124/3343037884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogistic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(input_dim, output_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogistic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummmary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegressionModel' object has no attribute 'summmary'"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegressionModel(N,n_classes) #(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_before_training = logistic_model.linear.bias\n",
    "w_before_training = logistic_model.linear.weight\n",
    "#bias_before_training.shape # shape = 10 car 1 biais associé à une classe, pour tous les x \n",
    "#w_before_training.shape # shape =  ([10, 256]) car pour chaque classe, on attribue 1 poid à chaque X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Définition des variables d'apprentissage, d'entrée et de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1 # epoch = le nombre de passages dans l'ensemble des données d'entraînement que le réseau a effectués, ici 513 max\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "batch_size = 100\n",
    "n_classes= 10\n",
    "\n",
    "X = torch.randn(N_batch, N)\n",
    "outputs = logistic_model(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défition des fonctions pour l'apprentissage des w et biais pour classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(X, y, learning_rate=learning_rate,batch_size=batch_size,num_epochs=num_epochs,betas=betas,verbose=False, **kwargs):\n",
    "\n",
    "    X_0 = torch.Tensor(X[:, None]) # rajoute une dimention de taille 1 : pk ? \n",
    "    labels = torch.Tensor(y[:, None]) # remplace les True/False par des 1 et 0, rajoute une dimention de taille 1 : pk ? \n",
    "    \n",
    "    loader = DataLoader(TensorDataset(X_0, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    N_batch = X.shape[0] # = 100\n",
    "    N = X.shape[1] # = 256 \n",
    "    n_classes = y.shape[1] # = 10\n",
    "    \n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad) # diminue le learning rate petit à petit de sorte à atteindre le minimum de la loss function / Adam utilise une moyenne à décroissance exponentielle des gradients passés afin de fournir une fréquence d'apprentissage adaptative. \n",
    "    \n",
    "    for epoch in range(int(num_epochs)): # on commence à 1 jusq'à 513 epoch, cad qu'il passe par tous les X_ 513 fois\n",
    "        logistic_model.train()\n",
    "        losses = [] \n",
    "        for X_, labels_ in loader: \n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            outputs = logistic_model(X_)  \n",
    "            loss = criterion(outputs, labels_) #calcul de la différence entre l'outpul de logistic_model et la classification effectuée pdt l'initialisation (PROBLEME ? on compare des proba à y=true/false)\n",
    "                                               \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "        \n",
    "            \n",
    "            losses.append(loss.item())   \n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\") \n",
    "\n",
    "    logistic_model.eval()\n",
    "    X_0, labels = torch.Tensor(X[:, None]), torch.Tensor(y[:, None]) #utile ? \n",
    "    outputs = logistic_model(X_0) #utile ? \n",
    "    loss = criterion(outputs, labels).item()\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la fonction __fit_data__ :\n",
    "- On commence par __définir les variables d'intéret et l'optimizer__. Lors de la définition de l'optimizer, on lui indique __quel paramètre il doit modfier__ pendant l'apprentissage et le learning rate. Ici les paramètres modifiés par l'optimizer sont les poids et biais associés aux entrées pour les différentes classes. L'optimizer Adam va modifier le poid et le biais des différentes classes pour que la classfication des entrées soit similaire à celle obtenue pendant l'initialisation. Pour cela, il fait ........... . La particularité de cet optimizer est qu'il diminue le learning rate en même temps que la différence diminue. Le learning rate correspond au nombre de pas necessaire pour atteindre le minimum de loss function- à approfondir - \n",
    "\n",
    "- En suite, __on commence l'entrainement__ à partir des données comprises dans loader (= X_0, labels // équivalent à : N_batch entrées de N valeurs (= X) et N_batch sorties de n_classes valeurs (= y)). Dans un premier temps, on prend les X, on les fait passer dans le logistic_model et on récupère dans outputs les probabilités de classification des N_batch entrées. Ensuite, on compare la classification effectuée par le logistic_model avec celle effectuée dans l'iniitialisation. Le criterion calcule la différence entre les deux par le calcule de la BCE et on stock cette différence dans loss. L'optimizer va modifier les w et biais associés aux entrées pour les différentes classes de sorte à diminuer au plus possible la loss function au cours de différentes iterations. \n",
    "\n",
    "- On demande d'imprimer les loss_function toutes les itérations multiples de 32 \n",
    "\n",
    "- ensuite on passe au __test du logistic_model__ et on renvoit la probabilté de classification des n_batch entrées ainsi que la loss_function après apprentissage des poids et biais optimaux pour que la classification se fasse comme dans l'initialisation. \n",
    "\n",
    "__On devrait ainsi obtenir des poids et biais similaires à ceux définis dans l'inialisation : C'est l'inversion__ \n",
    "\n",
    "On appelle la fonction get_data pour qu'elle nous renvoit les X, p et y.\n",
    "On appelle la fonction fit_data pour réaliser l'apprentissage des w et biais et la classification et qu'elle nous renvoit le modele logistic entrainé et les pertes au cours de l'apprentissage et après l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, p, y = get_data(W, seed=seed, N_batch=10000)\n",
    "logistic_model, loss = fit_data(X, y, verbose=True)\n",
    "#print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_after_training = logistic_model.linear.bias\n",
    "w_after_training = logistic_model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BIAIS')\n",
    "print('before training,', bias_before_training)\n",
    "print('after training,', bias_after_training) \n",
    "print('initialisation', W[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.plot(logistic_model.linear.bias.detach().numpy(), 'r')\n",
    "ax.plot(W[-1, :], 'b--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WEIGHT')\n",
    "print('before training,', w_before_training)\n",
    "print('after training,', w_after_training) \n",
    "print('initialisation : ', W[:-1,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_digit = 2\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.plot(W[:, i_digit], 'b--', lw=3)\n",
    "ax.plot(logistic_model.linear.weight[i_digit, :].detach().numpy(), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les biais et les poids ont changé après l'apprentissage pour se rapprocher de ceux de l'intialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un modele réaliste de courbes d'accord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de probabilités de firing des neurones, par classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je viens de modifier N et K pour aller plus vite, faire en sorte que les proba soient directement définis à partir des N et n_classes and stuff, à arranger encore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_width = 10\n",
    "phi = np.sqrt(5)/2 + 1/2\n",
    "N_trials = 2**11 # number of trials\n",
    "p_1 = .01 # average firing probability\n",
    "sigma = .51 # bandwidth\n",
    "\n",
    "N = 256 # nombre de valeurs de l'entrée\n",
    "n_classes = 10 # nombre de classes\n",
    "N_batch = 100 # nombre d'échantillons\n",
    "seed = 1973\n",
    "\n",
    "\n",
    "# several firing probabilities for each address\n",
    "p_2 = 1. - .75*np.linspace(-1, 1, N, endpoint=True)**2\n",
    "p_2 /= p_2.mean()\n",
    "p_2 *= p_1\n",
    "\n",
    "p_3 = 1. + 4.*np.linspace(-1, 1, N, endpoint=True)**2\n",
    "p_3 /= p_3.mean()\n",
    "p_3 *= p_1\n",
    "\n",
    "p_ = np.vstack((p_1*np.ones(N), p_2, p_3)).T\n",
    "\n",
    "# a simple circular function to generate patterns \n",
    "def von_mises(j, N, sigma):\n",
    "    p_ = np.exp( np.cos(2*np.pi* (np.linspace(0, 1, N, endpoint=False) -j/N)) / sigma**2)\n",
    "    p_ /= p_.sum()\n",
    "    return p_\n",
    " \n",
    "def stack(n_classes, N, sigma):\n",
    "    p_ = np.zeros((N, n_classes))\n",
    "    for classe in range(n_classes):\n",
    "        p_[:, classe] = von_mises(classe*N/n_classes, N, sigma)\n",
    "    return p_\n",
    "\n",
    "p_true = stack(n_classes, N, sigma)\n",
    "\n",
    "N_hyp = p_true.shape[-1] \n",
    "\n",
    "# représentation de différentes probabilités de décharge possibles en fonction de tous les neurones \n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(fig_width, fig_width/phi)) #fig, ax = plt.subplots(nb de plot, nb de x, taille de l'image largeur, longueur)\n",
    "\n",
    "ax[0].step(np.arange(N), p_3, color = 'green', label = 'p_3') #ax[0] = subplot 1, axes.step(x,y)\n",
    "ax[0].step(np.arange(N), p_2, color = 'orange', label = 'p_2')\n",
    "ax[0].hlines(p_1, 0, N, linestyles='--', label='p_1') #hlines (y, xmin, xmax), p1 = average firing probability\n",
    "ax[0].set_xlabel('address')\n",
    "ax[0].set_ylabel('probability of firing for each address \\n different distributions')\n",
    "\n",
    "# représentation de la vraie proba de firing\n",
    "for i_test in range(N_hyp):\n",
    "    ax[1].step(np.arange(N), (p_true[:, i_test].T)) #une courbe par pattern \n",
    "ax[1].set_ylabel('ground_true probability of firing for each address \\n true distributions (5 basis functions)')\n",
    "ax[1].set_xlabel('address')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On détermine l'activité de nos N neurones lorsqu'ils encodent un groupe polychrone. P_true correspond à la proba de firing de chaque neurone pour chaque classe, c'est notre ground_true, notre sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, p_true.shape, p_.shape, p.shape\n",
    "# les p sont différents : p_true(N,K) /// p_(p_1*N,p_1/2/3) /// p(n_batch,n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des poids à partir de la sortie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true = np.vstack((p_true, 0.1*np.ones((1, n_classes)))) #rajoute une ligne de 0.1, 10\n",
    "\n",
    "W = np.log(p_true/(1-p_true)) # on redéfinit les W à partir des probas qu'on vient de générer, voir si c'est un pb qu'on le def à partir de cos et pas à partir de sigmoides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de l'activité que les neurones ont lorsqu'ils encodent un groupe polychrone, donc __à partir de la sortie, on définit des poids pour classer nos entrées X__ dans les différentes classes : On connait l'activité des neurones quand un pattern d'interet est encodé, donc en définissant des poids à partir de cette activité, on sera en mesure de déterminer un __X*W plus élevé pour les entrées ayant un taux de décharge proche de la proba de firing true__ que pour celles ayant un X plus éloigné de p_true, ce qui augmentera leur chance de classification par rapport à celles ayant une activité plus élpoignée de p_true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, p, y= get_data(W, seed=seed, N_batch=1000000) \n",
    "X.shape, W.shape, e.shape, p.shape, p_true.shape, p_.shape, y.shape\n",
    "\n",
    "# pourquoi e(100,10) ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle la fonction __get_data__ avec de nouveaux W pour chaque classe, définis à partir de proabilité de firing des neurones. La fonction get_data va donc attribuer une nouvelle importance aux entrées (un nouveau e), qui, une fois transmformées selon la regression logistique seront classées. On récupère donc une nouvelle probabilité de classification (p = sigmoide de e) et une nouvelle classification (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(e[:,1],p[:,1]) \n",
    "plt.scatter(e[:,1],y[:,1]) \n",
    "\n",
    "# maintenant ça ne fait plus quelque chose de sigmoide quand on regarde p en fonction de e, problème ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1],p[:,1], alpha=1) #proba de e d'être classifié dans classe 1\n",
    "# Hplt.scatter(X[:,1],p_true[:,1], alpha=1) #proba de e d'être classifié dans classe 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(X, y, verbose=True) #apprentissage avec W fixé à partir de p \n",
    "print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réentraine notre réseau pour qu'il apprenne à classer les entrées de la même façon que pendant la nouvelle initialisation. Pour se rapprocher du nouveau y, il faudra qu'il apprenne les nouveaux poids qu'on a implémentés. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de l'apprentissage des poids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des poids appliqués à la classe 2 pdt l'initialisation et après apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_digit = 2 \n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.plot(W[:-1, i_digit], 'b--', lw=3)\n",
    "ax.plot(logistic_model.linear.weight[i_digit, :].detach().numpy(), 'r')\n",
    "ax.set_ylabel('weight')\n",
    "ax.set_xlabel('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des poids de toutes les classes pdt initialisation et après apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pourquoi les faire passer dans des sigmoides ? \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.plot(sigmoid(W[:-1, :]), '--', lw=3)\n",
    "ax.plot(sigmoid(logistic_model.linear.weight[:, :].detach().numpy().T), '-')\n",
    "ax.set_ylabel('weights')\n",
    "ax.set_xlabel('address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.plot(W[:-1, :], '--', lw=3)\n",
    "ax.plot(logistic_model.linear.weight[:, :].detach().numpy().T, '-')\n",
    "ax.set_ylabel('probability of firing for each address \\n true distributions (5 basis functions)')\n",
    "ax.set_xlabel('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là où il ya des fortes probabilités de firing, il y a de forst poids. Pour l'instant, les poids fixés et les poids appris ont la même allure et la même proportionalité mais ils n'ont pas le même ordre de grandeur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
